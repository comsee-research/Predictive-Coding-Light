{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47b4158c",
   "metadata": {},
   "source": [
    "# SUPERVISED LEARNING TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce00bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a50dcb",
   "metadata": {},
   "source": [
    "## WITHOUT DYNAMIC INHIBITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0051f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_net = \"...\" # network name\n",
    "name_type = \"cSTDP2\"\n",
    "name_file_inh = \"...\" # inhibitory file\n",
    "name_file = \"...\" # no inhibition file\n",
    "\n",
    "name_learnt = \"gestures\"\n",
    "name_random = \"gestures_rd\"\n",
    "name_feedforward = \"gesturesno\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94dafd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/.../\"+name_net+ \"/statistics/\"+name_file+\"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5649fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = \"/.../\"+name_net+ \"/statistics/\"+name_file+\"train/\"\n",
    "testing_dir = \"/.../\"+name_net+ \"/statistics/\"+name_file+\"test/\"\n",
    "\n",
    "training_dir_x = training_dir + \"x/\"\n",
    "training_dir_y = training_dir + \"y/\"\n",
    "\n",
    "testing_dir_x = testing_dir + \"x/\"\n",
    "testing_dir_y = testing_dir + \"y/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b3fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir_inh = \"/.../\"+name_net+ \"/statistics/\"+name_file_inh+\"train/\"\n",
    "testing_dir_inh = \"/.../\"+name_net+ \"/statistics/\"+name_file_inh+\"test/\"\n",
    "\n",
    "training_dir_xinh = training_dir_inh + \"x/\"\n",
    "training_dir_yinh = training_dir_inh + \"y/\"\n",
    "\n",
    "testing_dir_xinh = testing_dir_inh + \"x/\"\n",
    "testing_dir_yinh = testing_dir_inh + \"y/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c6ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332000d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(training_dir)\n",
    "os.mkdir(testing_dir)\n",
    "\n",
    "os.mkdir(training_dir_x)\n",
    "os.mkdir(training_dir_y)\n",
    "\n",
    "os.mkdir(testing_dir_x)\n",
    "os.mkdir(testing_dir_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7166ec68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "it_train = 0\n",
    "it_test = 0\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "for label in labels:\n",
    "    folder = dataset_dir + \"/\" + str(label) + \"/\"\n",
    "    files = natsorted(glob.glob(os.path.join(folder, '*.json')))\n",
    "    n_files = len(files)\n",
    "    thresh = round(80 * n_files / 100) \n",
    "    v = True\n",
    "    for ct, l in enumerate(range(n_files)):\n",
    "        with open(files[l]) as file:\n",
    "            params = json.load(file)\n",
    "        x = params[\"x\"]\n",
    "        y = params[\"y\"]\n",
    "        \n",
    "        if(ct+1 <= thresh):\n",
    "            np.save(training_dir_x + str(it_train) + \".npy\", x)\n",
    "            np.save(training_dir_y + str(it_train) + \".npy\", y)\n",
    "            it_train+=1\n",
    "        else:\n",
    "            np.save(testing_dir_x + str(it_test) + \".npy\", x)\n",
    "            np.save(testing_dir_y + str(it_test) + \".npy\", y)\n",
    "            it_test+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f7c1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16303ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "X_train_inh = []\n",
    "Y_train_inh = []\n",
    "\n",
    "X_test_inh = []\n",
    "Y_test_inh = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac24df39",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(train):\n",
    "    files_train_x = natsorted(glob.glob(os.path.join(training_dir_x, '*.npy')))\n",
    "    files_train_y = natsorted(glob.glob(os.path.join(training_dir_y, '*.npy')))\n",
    "\n",
    "    files_train_xinh = natsorted(glob.glob(os.path.join(training_dir_xinh, '*.npy')))\n",
    "    files_train_yinh = natsorted(glob.glob(os.path.join(training_dir_yinh, '*.npy')))\n",
    "\n",
    "else:\n",
    "    files_test_x = natsorted(glob.glob(os.path.join(testing_dir_x, '*.npy')))\n",
    "    files_test_y = natsorted(glob.glob(os.path.join(testing_dir_y, '*.npy')))\n",
    "\n",
    "    files_test_xinh = natsorted(glob.glob(os.path.join(testing_dir_xinh, '*.npy')))\n",
    "    files_test_yinh = natsorted(glob.glob(os.path.join(testing_dir_yinh, '*.npy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa87a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(train):\n",
    "    print(len(files_train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60477742",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not train):\n",
    "    print(len(files_test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646b8d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_kernels = True\n",
    "which = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e7351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_simple = 5184\n",
    "n_complex = 1152\n",
    "def CutKernel(a, desc):\n",
    "    if (a == 0): # all\n",
    "        X_simple = []\n",
    "        X_complex = []\n",
    "        for e in range(0, len(desc), n_simple + n_complex):\n",
    "            X_simple.append(desc[e: e + n_simple])\n",
    "            X_complex.append(desc[e + n_simple: e + n_simple + n_complex])\n",
    "        X_simple = np.array(X_simple).flatten()\n",
    "        X_complex = np.array(X_complex).flatten()\n",
    "        return X_simple, X_complex\n",
    "    X_ = []\n",
    "    for e in range(0, len(desc), n_simple + n_complex):\n",
    "        if(a == 1): # simple\n",
    "            X_.append(desc[e: e + n_simple])\n",
    "        if(a == 2): # complex\n",
    "            X_.append(desc[e + n_simple: e + n_simple + n_complex])\n",
    "    X_ = np.array(X_).flatten()\n",
    "    return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eda6278",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_nz = []\n",
    "test_nz = []\n",
    "skip = 1\n",
    "lim_files_training = 40000 \n",
    "lim_files_val = 2000 \n",
    "lim_files_testing = 50000 \n",
    "ct_zero = []\n",
    "\n",
    "if(train):\n",
    "    for i in range(0,len(files_train_x)):\n",
    "        if(i%skip==0 and len(X_train)<lim_files_training):\n",
    "            if(np.max(np.load(files_train_x[i],allow_pickle=True))!=0):\n",
    "                if(not all_kernels):\n",
    "                    X_ = np.load(files_train_x[i],allow_pickle=True).flatten();\n",
    "                    X__ = []\n",
    "                    for e in range(0,len(X_),5184):\n",
    "                        X__.append(np.reshape(X_[e:e+5184], (81,64)))\n",
    "                    X_train.append(np.mean(np.array(X__),axis=1).flatten() / np.max(np.mean(np.array(X__),axis=1).flatten()))\n",
    "                else:\n",
    "                    sa = np.load(files_train_y[i]).flatten()\n",
    "                    var = np.load(files_train_x[i],allow_pickle=True).flatten() \n",
    "                    var_inh = np.load(files_train_xinh[i],allow_pickle=True).flatten() \n",
    "                    if(which !=0):\n",
    "                        var = CutKernel(which, var)\n",
    "                        var_inh = CutKernel(which, var_inh)\n",
    "                        if(np.max(var)!=0):\n",
    "                            X_train.append(var)\n",
    "                            size_init = int(np.sum(X_train[-1]))\n",
    "                            size_inhib = int(np.sum(var_inh) / divide)\n",
    "                            for value in range(size_init - size_inhib):\n",
    "                                replace = np.where(X_train[-1]!=0)[0]\n",
    "                                length = len(replace)\n",
    "                                v = np.random.randint(length)\n",
    "                                X_train[-1][replace[v]] = X_train[-1][replace[v]] - 1\n",
    "                            train_nz.append(np.sum(X_train[-1]))\n",
    "                    else:\n",
    "                        var_simple, var_complex = CutKernel(which, var)\n",
    "                        var_inh_simple, var_inh_complex = CutKernel(which, var_inh)\n",
    "                        if(np.max(var_simple)!=0):\n",
    "                            size_init = int(np.sum(var_simple))\n",
    "                            size_inhib = int(np.sum(var_inh_simple) / divide)\n",
    "                            for value in range(size_init - size_inhib):\n",
    "                                replace = np.where(var_simple!=0)[0]\n",
    "                                length = len(replace)\n",
    "                                v = np.random.randint(length)\n",
    "                                var_simple[replace[v]] = var_simple[replace[v]] - 1\n",
    "                        if(np.max(var_complex)!=0):\n",
    "                            size_init = int(np.sum(var_complex))\n",
    "                            size_inhib = int(np.sum(var_inh_complex) / divide)\n",
    "                            for value in range(size_init - size_inhib):\n",
    "                                replace = np.where(var_complex!=0)[0]\n",
    "                                length = len(replace)\n",
    "                                v = np.random.randint(length)\n",
    "                                var_complex[replace[v]] = var_complex[replace[v]] - 1\n",
    "                        X_train.append(np.concatenate( (var_simple, var_complex), axis = 0))\n",
    "                        train_nz.append(np.sum(X_train[-1])) \n",
    "            else:\n",
    "                print(\"File number {} skipped because it was equal to 0.\".format(i+1))\n",
    "                continue\n",
    "            if(np.max(var!=0)):\n",
    "                Y_train.append(np.load(files_train_y[i]).flatten())\n",
    "\n",
    "                print(\"File number (train) {0}/{1}\".format(i+1, len(files_train_x)))\n",
    "\n",
    "else:\n",
    "    for i in range(0, len(files_test_x)):    \n",
    "        if(i<len(files_test_x) and len(X_test)<lim_files_testing):\n",
    "                if(np.max(np.load(files_test_x[i]))!=0):\n",
    "                    if(not all_kernels):\n",
    "                        X_ = np.load(files_test_x[i],allow_pickle=True).flatten();\n",
    "                        X__ = []\n",
    "                        for e in range(0,len(X_),5184):\n",
    "                            X__.append(np.reshape(X_[e:e+5184], (81,64)))\n",
    "                        X_test.append(np.mean(np.array(X__),axis=1).flatten() / np.max(np.mean(np.array(X__),axis=1).flatten()))\n",
    "                    else:\n",
    "                        sa = np.load(files_test_y[i]).flatten()\n",
    "                        var = np.load(files_test_x[i],allow_pickle=True).flatten() \n",
    "                        var_inh = np.load(files_test_xinh[i],allow_pickle=True).flatten() \n",
    "                        if(which!=0):\n",
    "                            var = CutKernel(which, var)\n",
    "                            var_inh = CutKernel(which, var_inh)\n",
    "                            if(np.max(var)!=0):\n",
    "                                X_test.append(var)\n",
    "                                size_init = int(np.sum(X_test[-1]) )\n",
    "                                size_inhib = int(np.sum(var_inh) / divide)\n",
    "                                init_replace = np.where(X_test[-1]!=0)[0]\n",
    "                                for value in range(size_init - size_inhib):\n",
    "                                    replace = np.where(X_test[-1]!=0)[0]\n",
    "                                    length = len(replace)\n",
    "                                    v = np.random.randint(length)\n",
    "                                    X_test[-1][replace[v]] = X_test[-1][replace[v]] - 1\n",
    "                                test_nz.append(np.sum(X_test[-1]))\n",
    "                        else:\n",
    "                            var_simple, var_complex = CutKernel(which, var)\n",
    "                            var_inh_simple, var_inh_complex = CutKernel(which, var_inh)\n",
    "                            if(np.max(var_simple)!=0):\n",
    "                                size_init = int(np.sum(var_simple))\n",
    "                                size_inhib = int(np.sum(var_inh_simple) / divide)\n",
    "                                for value in range(size_init - size_inhib):\n",
    "                                    replace = np.where(var_simple!=0)[0]\n",
    "                                    length = len(replace)\n",
    "                                    v = np.random.randint(length)\n",
    "                                    var_simple[replace[v]] = var_simple[replace[v]] - 1\n",
    "                            if(np.max(var_complex)!=0):\n",
    "                                size_init = int(np.sum(var_complex))\n",
    "                                size_inhib = int(np.sum(var_inh_complex) / divide)\n",
    "                                for value in range(size_init - size_inhib):\n",
    "                                    replace = np.where(var_complex!=0)[0]\n",
    "                                    length = len(replace)\n",
    "                                    v = np.random.randint(length)\n",
    "                                    var_complex[replace[v]] = var_complex[replace[v]] - 1\n",
    "                            X_test.append(np.concatenate( (var_simple, var_complex), axis = 0))\n",
    "                            test_nz.append(np.sum(X_test[-1])) \n",
    "\n",
    "                else:\n",
    "                    print(\"File number {} skipped because it was equal to 0.\".format(i+1))\n",
    "                    continue\n",
    "                if(np.max(var!=0)):\n",
    "                    Y_test.append(np.load(files_test_y[i]).flatten())\n",
    "                    print(\"File number (test) {0}/{1}\".format(i+1, len(files_test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c36f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(train):\n",
    "    print(\"min number = {}, median number = {}, max number = {}\".format(np.min(train_nz), np.median(train_nz), np.max(train_nz)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d616477",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not train):\n",
    "    print(\"min number = {}, median number = {}, max number = {}\".format(np.min(test_nz), np.median(test_nz), np.max(test_nz)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b9da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(train):\n",
    "    X_train = np.array(X_train) #/np.max(np.array(X_train))\n",
    "    Y_train = np.array(Y_train) \n",
    "else:\n",
    "    X_test = np.array(X_test) #/np.max(np.array(X_test))\n",
    "    Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5694a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67501c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.count_nonzero(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6087164",
   "metadata": {},
   "source": [
    "### WITHOUT NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8091b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not train):\n",
    "    import joblib\n",
    "    path_home = \".../\"+name_net+\"/\" + name_type + \"/statistics/\"\n",
    "    classif_path = path_home + name_file_inh + \"... .sav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e3e305",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not train):\n",
    "    classif = joblib.load(classif_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528f95e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(train):\n",
    "    classif = svm.LinearSVC(max_iter=10000, C=1).fit(X_train, np.ravel(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac907988",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(train):\n",
    "    counter=0\n",
    "    good_number=0\n",
    "    count = np.zeros((len(labels)))\n",
    "    tot_number = np.zeros((len(labels)))\n",
    "    v = [0, 1, 2]\n",
    "    for feature in X_train:\n",
    "        pred=classif.predict([feature]) \n",
    "        number = Y_train[counter]\n",
    "        if pred.item()==number:\n",
    "            good_number+=1\n",
    "            count[np.where(np.array(labels)==number)[0][0]]+=1\n",
    "        counter+=1\n",
    "        tot_number[np.where(np.array(labels)==number)[0][0]]+=1\n",
    "    print(\"The percentage of correct classification is: {}%.\".format(good_number*100/counter))\n",
    "    for num, val in enumerate(labels):\n",
    "        print(\"The percentage of correct classification for number {} is: {}%\".format(val, count[num]*100/tot_number[num]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a27f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(train):\n",
    "    import joblib\n",
    "    path_home = \"/.../\"+name_net+ \"/statistics/\"\n",
    "    classif_path = path_home + name_file_inh + \"learnt.sav\"\n",
    "\n",
    "    joblib.dump(classif, classif_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eeb40f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(not train):\n",
    "    counter=0\n",
    "    good_number=0\n",
    "    count = np.zeros((len(labels)))\n",
    "    tot_number = np.zeros((len(labels)))\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for feature in X_test:\n",
    "        pred=classif.predict([feature]) \n",
    "        number = Y_test[counter]\n",
    "        if pred.item()==number:\n",
    "            good_number+=1\n",
    "            count[np.where(np.array(labels)==number)[0][0]]+=1\n",
    "        counter+=1\n",
    "        tot_number[np.where(np.array(labels)==number)[0][0]]+=1\n",
    "        y_true.append(number)\n",
    "        y_pred.append(pred.item())\n",
    "    for num, val in enumerate(labels):\n",
    "        print(\"The percentage of correct classification for number {} is: {}%\".format(val, count[num]*100/tot_number[num]))\n",
    "\n",
    "    print()\n",
    "    print(\"The percentage of correct classification is: {}%.\".format(good_number*100/np.sum(tot_number)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
